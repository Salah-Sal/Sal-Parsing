# PaddleOCR Tier 1 — Basic OCR

**Paper:** 2512.24601v2.pdf (pages 1–9)
**Time:** 277.9s (OCR) + 0.8s (init)
**Model:** PP-OCRv5 mobile det + en mobile rec
**DPI:** 150

---


## Page 1

Recursive Language Models
Alex L. Zhang 1 Tim Kraska 1 Omar Khattab 1
Abstract
GPT-5
100
C
We study allowing large language models (LLMs)
S-NIAH
to process arbitrarily long prompts through the
80
lens of inference-time scaling. We propose Re-
(%) oos
60
2807 2n 87
cursive Language Models (RLMs), a general
40
inference paradigm that treats long prompts as
20
OOLONG
part of an external environment and allows the
LLM to programmatically examine, decompose,
OOLONG-Pairs
0
and recursively call itself over snippets of the
8K
1M
16k
33k
66k
131k
262k
RLM(GPT-5)
524k
prompt. We find that RLMs can successfully
process inputs up to two orders of magnitude
100
S-NIAH
beyond model context windows and, even for
80
^
shorter prompts, dramatically outperform the
(%) sos
-
TZTZSTXI
60
L
OOLONG-Pairs
quality of vanilla frontier LLMs and common
long-context scaffolds across four diverse long-
40
OOLONG
context tasks while having comparable cost. At
20
a small scale, we post-train the first natively
recursive language model. Our model, RLM-
0
BK
16k
33k
66k
131k
262k
524k
1M
Qwen3-8B, outperforms the underlying Qwen3-
Input Context Length (log scale)
8B model by 28.3% on average and even ap-
Figure 1. A comparison of GPT-5 and a corresponding RLM using
proaches the quality of vanilla GPT-5 on three
GPT-5 on three long-context tasks of increasing complexity: S-
long-context tasks. Code is available at ht t ps :
NIAH, OOLONG, and OOLONG-Pairs. For each task, we scale
//github.com/alexzhang13/rlm.
the input length from 213 to 218. GPT-5 performance degrades
significantly as a function of both input length and task complexity,
while the RLM maintains strong performance. Inputs beyond the
1. Introduction
red region do not fit in GPT-5's context window of 272K tokens,
but the RLM handles them effectively. Additional experiments
Frontier reasoning models have limited context windows
across other models and benchmarks are in §3.
and, even within their limits, tend to exhibit context
rot (Hong et al., 2025), a phenomenon illustrated in Fig-
resulting not only in empirical gains but also additional the-
ure 1 where quality degrades steeply as prompts get longer.
oretical expressive power (Merrill & Sabharwal, 2024) com-
Though we expect context lengths to steadily rise through
pared to vanilla Transformers. Though most inference-time
improvements to training, architecture, and infrastructure,
methods for dealing with long context are task-specific (Wu
we are interested in whether it is possible to scale the context
et al., 2021; Chang et al., 2024), the most popular general
size of general-purpose LLMs by orders of magnitude. This
approach is context condensation or compaction (Khattab
is increasingly urgent as LLMs begin to be widely adopted
et al., 2021; Smith, 2025; OpenAI, 2025b; Wu et al., 2025),
for long-horizon tasks, in which they must routinely process
where context from user requests or agent trajectories is
tens if not hundreds of millions of tokens.
repeatedly summarized once it exceeds a length threshold.
Unfortunately, compaction is rarely expressive enough for
We study this question through the lens of scaling inference-
time compute. We are inspired by the way that reasoning
tasks that require dense access throughout the prompt. It
presumes that some details that appear early in the prompt
models have become the fundamental interface to LLMs,
can safely be forgotten to make room for new content.
1MIT CSAIL, Cambridge, MA, USA. Correspondence to: Alex
L. Zhang, Omar Khattab <altzhang@mit.edu, okhattab@mit.edu>.
We introduce Recursive Language Models (RLMs), a
general-purpose inference paradigm for dramatically scaling
Preprint. January 29, 2026.
the effective input and output lengths of LLMs. The key

## Page 2

Recursive Language Models
RLM (root / depth=0)
Language Model (LM)
 Environment E
A
*
Prompt
You are reading an extremely long book. Can you list
out all which items that were made before the Great.
RLM (depth=1)
prompt loaded as variable
Prompt
In Chapter 1, find all items
print(prompt[:100])
that are listed to belong to
In[1]
people with...
You are reading an extremely long book. Can you list out all items that
1
were made before the Great Catastrophe?\n\n**Chapter 1**.\n Bob found
Language Model (LM)
solace in walking through the forest. It was a strange time, rife with...
Out[1]
part1, part2 = prompt.split("Chapter 2")
Thb lepa
RLM (depth=1)
pre_cata = IIm_query(f"in Chapter 1, find.. {part1). ")
post_cata = Ilm_query(f™Look for the.. {part2}.")
Prompt
In[2]
Look for the items in these
• • •
Sub-Response
sections that explicitly say
Herod's ring..
they were found...
print(FINAL_ANSWER)
*
In[N]
Language Model (LM)
•From chapter 1, Bob was the only character that was noted to be alive
and carrying a flask before the Great Catastrophe. In chapter 3, Alice
uncovers a ring belonging to the fallen King Herod, and later...
Out[N]
Final Response
From chapter 1, Bob was the only character that was noted to be alive and carrying a
flask before the Great Catastrophe. In chapter 3, Alice uncovers a ring belonging to
the fallen King Herod, and later in chapter 5, Alice finds..
Figure 2. A Recursive Language Model (RLM) treats prompts as part of the environment. It loads the input prompt as a variable inside a
REPL environment E and writes code to peek into, decompose, and invoke itself recursively over programmatic snippets of the variable.
insight is that arbitrarily long user prompts should not be
icapped by the underlying LLM's limited output lengths
fed into the neural network (e.g., Transformer) directly but
because they are designed to verbalize sub-calls autoregres-
should instead be treated as part of the environment that the
sively rather than producing them programmatically.
LLM is tasked to symbolically and recursively interact with.
We evaluate RLMs using a frontier closed model (GPT-
As Figure 2 shows, an RLM exposes the same external
5; Singh et al. 2025) and a frontier open model (Qwen3-
interface as an LLM or a reasoning model: it accepts a string
Coder-480B-A35B; Qwen Team 2025b) across four tasks
prompt of arbitrary structure and produces a string response.
with varying levels of complexity: deep research (Chen
Given a prompt P, the RLM initializes a Read-Eval-Print
et al., 2025), information aggregation (Bertsch et al., 2025),
Loop (REPL) programming environment in which P is set
code repository understanding (Bai et al., 2025), and a syn-
as the value of a variable. It then offers the LLM general
thetic pairwise reasoning task where even frontier models
context about the REPL environment (e.g., the length of the
fail catastrophically. We compare RLMs against direct LLM
string P), and permits it to write code that peeks into and
calls as well as context compaction, retrieval tool-use agents,
decomposes P, and to iteratively observe any side effects
and code-generation agents.
from execution. Crucially, RLMs encourage the LLM to
We find that RLMs demonstrate extremely strong perfor-
understand, transform, and execute the input prompt by
mance even at the 10M+ token scale, and substantially out-
writing symbolic programs that invoke the LLM itself on as
perform all other approaches at long-context processing, in
many slices of the input as necessary.
many cases by double-digit percentage gains while main-
By treating the prompt itself as an external object and en-
taining comparable cost. In particular, as demonstrated
abling symbolic recursion, RLMs tackle limitations of ex-
in Figure 1, RLMs exhibit far less severe degradation for
pressive power in recent work on coding agents, retrieval
longer contexts and more sophisticated tasks.
agents, and sub-agent delegation. In particular, prior coding
Finally, at a small scale, we post-train the first natively
agents and retrieval agents treat some designated external
recursive language model, demonstrating that RLMs can be
data source (e.g., a filesystem or a corpus of search docu-
improved quickly with little additional training. While a
ments) as an environment for fetching snippets. However,
small open model (Qwen3-8B; Yang et al. 2025) struggles to
they can only fill up the underlying LLM's context window
solve long context tasks even in an RLM scaffold, our simple
with snippets before breaking down. Similarly, prior self-
delegation approaches (Anthropic, 2025; Sentient AI, 2025;
general-purpose training recipe uses only 1,000 samples
from unrelated domains to improve its performance by a
Schroeder et al., 2025; Sun et al., 2025) allow LLMs to
median of 28.3% across the four evaluation tasks.
invoke themselves as sub-agents. However, they are hand-
2

## Page 3

Recursive Language Models
2. Recursive Language Models
Algorithm 1 A recursive language model, around LLM M
Input: prompt P
Given a base neural language model M with maximum
Output: response Y
context size K, a Recursive Language Model (RLM) is
state←InitREPL(prompt=P)
an inference-time scaffold around M that treats the user
state←AddFunction(state, sub_RLMM)
prompt as part of the environment without giving up the
hist←[Metadata(state)]
ability to densely process its content through different calls
while True do
to M. Given an arbitrary-length prompt string P  Σ*, an
code ←
LLM(hist)
RLM interacts with a persistent external environment E and
returns a response string Y  Σ* (Figure 2). We would like
(state, stdout)←
REPL(state, code)
effectively unbounded input tokens (|P| > K), unbounded
hist ←hist | code |Metadata(stdout)
output tokens, and an unbounded semantic horizon, e.g. the
if s t a te [ Fi nal ] is set then
ability to do Ω(|P|) or Ω(|P|2) semantic work.
L return state[Final]
Algorithm 1 describes how an RLM achieves this. Given
a prompt P, the RLM initializes a persistent REPL pro-
Algorithm 2 Alternate scaffold with standard (poor) design
gramming environment with a variable containing the user
choices for prompts, sub-calls, and code execution
prompt as a string and a function for invoking a sub-RLM
Input: prompt P
with a new prompt. Then, it starts the RLM loop. In the first
Output: response Y
iteration, the algorithm invokes the root neural model M
actions← {Finish, Exec, Search, sub_LLMM}
with only (constant-size) metadata about the user prompt,
hist ← [Metadata(actions), P]
//
Flaw #1
like its length, a short prefix, and how to access parts of it.
while True do
The root is instructed via prompting (Appendix C) and/or
(action,val)←
LLM(hist)
fine-tuning (Appendix A) to operate like an RLM: that is,
if action is Finish then
to generate code that helps it understand and transform its
return val
//
Flaw #2
parts of its prompt P, and to build up intermediate values
out ←
RUN(action, val)
//
Flaw #3
and the final response into new variables, potentially by
hist ←hist (action, val,out)
invoking the sub-RLM within loops. In Section 4, we find
if Tok (hist) > K then
that existing LLMs can be prompted to do this and that
L hist ← Compact(hist)
training an 8B model to be natively recursive is promising.
Each iteration of the RLM loop executes code in the REPL,
updates REPL state (intermediate variables), and collects
without copying text into the root context window. Instead,
in st dout any printed text. Only (constant-size) metadata
ineffective Algorithm 2 starts by putting the user prompt
about st dout, like a short prefix and length, is appended
P into the LLM context window (h i st) and thus inherits
to M's history for the next iteration.1 Once the RLM sets
the window limitations of M and falls back to heuristics
the variable F i na l inside the REPL, iteration stops and the
like context compaction. Even though the scaffold can ac-
value in F ina l is returned as the response.
cess external data with, say, a Sea r ch action or filesystem
RLMs make three simple design choices that are missing
access, it is fatally bounded with respect to user input.
from existing scaffolds. To highlight these, we include
Second, ineffective Algorithm 2 asks M to autoregressively
Algorithm 2 to illustrate a deceptively "similar" algorithm
generate the output directly, via a F in i sh action. This may
that is far less expressive. Both algorithms support some
seem innocuous, but it means that it also cannot generate
notion of sub-calls, external objects, and code execution, but
longer outputs than the context window of M permits.
they differ in terms of where the prompt and intermediate
Third, and perhaps most importantly, an RLM requires sym-
values live and where recursion occurs.
bolic recursion. That is, code running inside E must be
First, an RLM must give the underlying LLM M a symbolic
able to invoke M on programmatically constructed trans-
handle to the user prompt P, so the model can manipulate it
formations of P (e.g., inside arbitrarily large loops), storing
1This is key: it forces M to rely on variables and sub-calls to
intermediate results symbolically. Though Algorithm 2 in-
manage long strings instead of polluting its window. In principle,
cludes both a code execution action and a "sub-LLM" action
if we trim each turn to c tokens, we will have at most K/c root
separately, it is not able to invoke the sub-LLM programmat-
iterations, each of which can launch arbitrarily many sub-calls.
ically and hence can only delegate a few explicitly verbalized
This is not a fundamental limitation, e.g. one could move the root
tasks rather than writing short programs that can, say, loop
horizon itself into a variable, but we typically want to limit the
over slices of the prompt and launch Ω(|P |) or even Ω(|P |2)
iterations at any level of recursion irrespective.
processes to understand or transform all parts of P.
3

## Page 4

Recursive Language Models
3. Scaling Long Context Tasks
questions with semantic labels. Each task requires using
nearly all entries of the dataset, and therefore scales linearly
We hypothesize that the effective context window (Hsieh
in processing complexity relative to the input length.
et al., 2024; Goldman et al., 2025; Hong et al., 2025) of an
LLM cannot be understood independently of the specific
OOLONG-Pairs. We modify the t rec_coarse split of
task. That is, more "complex" problems will exhibit degra-
OOLONG to include 20 new queries that specifically require
dation at even shorter lengths than simpler ones. Because
aggregating pairs of chunks to construct the final answer.
of this, we must characterize tasks in terms of how their
We report F1 scores over the answer. Each task requires
complexity scales with prompt length.
using nearly all pairs of entries of the dataset, and therefore
requires processing quadratically-many items relative to the
For example, needle-in-a-haystack (NIAH) problems gener-
input length. In Appendix D.1, we provide all queries in
ally keep 'needles' constant as prompt length is scaled. As a
this benchmark.
result, frontier models can now reliably solve these tasks in
RULER (Hsieh et al., 2024) in the 1M+ token settings but
LongBench-v2 CodeQA (Bai et al., 2025). A multi-choice
struggle at far shorter lengths on OOLONG (Bertsch et al.,
code repository understanding split from LongBench-v2 that
2025), a task where the answer depends explicitly on almost
is challenging for modern frontier models. We report the
every line in the prompt.2
score as the percentage of correct answers. Each instance
requires reasoning over a fixed number of files in a codebase
3.1. Tasks
to find the right answer.
We design our evaluation around tasks where we can vary
3.2. Methods and Baselines
the lengths of the prompts, so we can consider problems
whose difficulties scale differently with context length.
We compare RLMs against commonly used task-agnostic
inference methods, using two modern LMs, GPT-5 with
S-NIAH. Following the single needle-in-the-haystack task
medium reasoning (Singh et al., 2025) and default sampling
in RULER (Hsieh et al., 2024), we consider a set of 50
parameters, and Qwen3-Coder-480B-A35B (Yang et al.,
single tasks that require finding a specific phrase or number
2025) using the sampling parameters described in Qwen
in a large set of unrelated text. Here, the information being
Team (2025b). For Qwen3-Coder-480B-A35B, we compute
sought scales as O(1) with respect to input length.
costs based on the compute provider Fireworks (Fireworks
BrowseComp-Plus (1K documents) (Chen et al., 2025).
AI, 2025). In addition to evaluating the base model on all
A multi-hop question-answering benchmark for DeepRe-
tasks, we also evaluate the following methods and baselines:
search (OpenAI, 2025a) questions that requires reasoning
CodeAct (+ BM25). We compare directly to a Code-
over multiple different documents. The benchmark provides
Act (Wang et al., 2024) agent that can execute code inside of
a verified offline corpus that is guaranteed to contain gold,
a ReAct (Yao et al., 2023) loop. Unlike an RLM, CodeAct
evidence, and hard negative documents for each question.
does not offload the user prompt to the code environment,
Following Sun et al. (2025), we use 150 randomly sampled
and instead provides it directly to the LM. Furthermore, fol-
instances as our evaluation set; we provide 1000 randomly
lowing Jimenez et al. (2024); Chen et al. (2025), we equip
chosen documents as input, in which the gold and evidence
this agent with a BM25 (Robertson & Zaragoza, 2009) re-
documents are guaranteed to exist. We report the percentage
triever that indexes the input context for tasks where a re-
of correct answers. The answer to each task requires piec-
triever is appropriate.
ing together information from several documents, making
this harder than S-NIAH despite also requiring a constant
CodeAct with sub-calls. To specifically ablate offloading
number of documents.
the context as a variable in the REPL, we evaluate a Code-
Act (Wang et al., 2024) baseline with the ability to invoke
OOLONG (Bertsch et al., 2025). A long reasoning bench-
sub-LM calls. Compared to RLMs, this method loads the
mark that requires transforming chunks of the input seman-
context directly into the model.
tically, then aggregating these chunks to form a final an-
swer. We report scoring based on the original paper, which
Summary agent. Following Sun et al. (2025); Wu et al.
scores numerical answers as s core(¯y) = 0.75|y−¯| and
(2025); Yu et al. (2025), we consider an iterative agent that
other answers as exact match. We focus specifically on the
compacts the context as it is filled. For example, given a
trec_coarse split, a set of 50 tasks over a dataset of
corpus of documents, it will iteratively accumulate the doc-
uments and summarize when full. In cases where a single
2This helps explain the patterns seen in Figure 1 earlier: GPT-5
document exceeds the model window, the agent will chunk it
scales effectively on the S-NIAH task, where the needle size is
to fit within the model context window and invoke the same
constant despite longer prompts, but shows faster degradation
at increasingly shorter context lengths on the linear-complexity
strategy over these chunks. For the GPT-5 experiments,
OOLONG and the quadratic-complexity OOLONG-Pairs.
due to the extremely high cost of applying this strategy to
4

## Page 5

Recursive Language Models
Table 1. Performance comparison of different methods across long-context benchmarks of varying complexity. In gray is the average API
cost ± the standard deviation of each method on each task. * indicates runs where a method (sometimes) ran into input context limits.
Provider costs were computed under OpenAI for GPT-5 and Fireworks for other models. Non-zero scores are rounded to at least 0.1.
Model
CodeQA
BrowseComp+ (1K) OOLONG
OOLONG-Pairs
Task Length N (tokens)
23K-4.2M
6M-11M
131K
32K
GPT-5(with RLM sub-calls to GPT-5-mini)
Base Model
24.0*
($0.13 ± $0.07)
0.0*
 (N/A) ± (N/A)
44.0(s0.14 ± $0.02)
0.1 ($0.16 ± $0.10)
CodeAct (+ BM25)
22.0* ($0.06 ± $0.08)
51.0($0.71 ± $1.20)
38.0($0.61 ± $1.06)
24.7($0.75 ± $0.43)
CodeAct (+ sub-calls)
24.0* ($0.06 ± $0.08)
0.0* (N/A) ± (N/A)
40.0($0.85 ± $1.27)
28.4 ($1.11 ± $0.62)
Summary agent
58.0($1.31 ± $1.46)
70.5($0.57 ± $0.10)
46.0($0.13 ± $0.01)
0.1 ($0.13 ± $0.09)
RLM
62.0($0.11 ± $0.10)
91.3($0.99 ± $1.22)
56.5($0.43 ± $0.85)
58.0($0.33 ± $0.20)
RLM (no sub-calls)
58.0($0.18 ± $0.56)
88.0($0.44 ± $0.90)
36.0($0.37 ± $0.42)
43.9($0.69 ± $1.16)
Qwen3-Coder-480B-A35B
Base Model
20.0* ($0.13 ± $0.08)
0.0* (N/A) ± (N/A)
36.0($0.06 ± $0.00)
0.1($0.05 ± $0.01)
CodeAct (+ BM25)
24.0* ($0.17 ± $0.08)
12.7 ($0.39 ± $0.50)
38.0($1.51 ± $1.09)
0.3($1.54 ± $0.35)
CodeAct (+ sub-calls)
26.0*
($0.28 ± $0.30)
0.0* (N/A) ± (N/A)
32.0($1.83 ± $1.14)
0.1 ($1.49 ± $0.46)
Summary agent
50.0($1.26 ± $1.50)
38.0($8.98 ± $2.12)
44.1 ($0.15 ± $0.01)
0.31 ($0.05 ± $0.00)
RLM
56.0($0.92 ± $1.23)
44.7 ($0.84 ± $0.63)
48.0($0.61 ± $0.49)
23.1($1.02 ± $0.52)
RLM (no sub-calls)
66.0($0.18 ± $0.58)
46.0($0.82 ± $0.69)
43.5($0.32 ± $0.13)
17.3($1.77 ± $1.23)
Qwen3-8B
Base Model
4.0* ($0.01 ± $0.00)
0.0* (N/A) ± (N/A)
0.0* (N/A) ± (N/A)
0.1 ($0.01 ± $0.00)
RLM
26.0($0.04 ± $0.13)
2.0 ($0.03 ± $0.06)
24.0($0.19 ± $0.26)
4.3($0.05 ± $0.05)
RLM (fine-tuned)
32.0($0.02 ± $0.02)
14.0($0.01 ± $0.03)
32.0(s0.04 ± $0.09)
5.2($0.02 ± $0.02)
millions of tokens, we use GPT-5-nano for compaction and
sub-call model is roughly similar to being a general purpose
GPT-5 to provide the final answer.
reasoning model, so we can make the training much more
tractable (and seemingly short-horizon) at small scale by fo-
RLM with REPL. We implement an RLM with a Python
REPL environment, which loads a module for querying a
cusing on improving the root model's ability to manipulate
the REPL and to launch recursive calls. We provide more
sub-LM and uses a system prompt presented in Appendix C.
training details in Appendix A.
For the GPT-5 experiments, we use GPT-5-mini for the
recursive LMs and GPT-5 for the root LM, as we found this
choice to strike a good balance between the capabilities of
4. Results and Discussion
RLMs and the cost of the recursive calls. We notate a RLM
Table 1 reports our main results. We additionally explore
using a model as RLM(model), e.g. RLM(GPT-5).
how vanilla frontier model performance and RLM perfor-
RLM with REPL, no sub-calls. We provide an ablation
mance degrades as input contexts grow in Figure 1.
of our method, in which the prompt is loaded in a REPL
Observation 1: RLMs can scale to the 10M+ token
environment without the ability to invoke sub-LM calls.
regime and can outperform base LMs and existing task-
Finetuning. To create RLM-Qwen3-8B, we finetune
agnostic agent scaffolds on long context tasks. Across all
Qwen3-8B on 1,000 filtered trajectories of Qwen3-Coder-
tasks, RLMs demonstrate strong performance on prompts
480B-A35B as an RLM with Qwen3-8B sub-calls on Long-
well beyond the effective context window of a frontier LM,
BenchPro (Chen et al., 2026) tasks. We use sampling pa-
outperforming base models and common long-context scaf-
rameters described in Qwen Team (2025a), and evaluate the
folds by up to 2× the performance while maintaining com-
fine-tuned RLM-Qwen3-8B as an RLM on our long context
parable or cheaper average token costs. Notably, RLMs
tasks. The key insight for training is that being an effective
scale well beyond the base models' context window. For
5

## Page 6

Recursive Language Models
GPT-5
Qwen3-Coder-480B
Base Model (GPT-5)
4
Base Model (Qwen3-Coder-480B)
RLM(GPT-5) with REPL
RLM(Qwen3-Coder-480B) with REPL
3
70
RLM(GPT-5) with REPL (no sub-calls)
3
7Z
RLM(Qwen3-Coder-480B) with REPL (no sub-calls)
($) 1so)
Summary Agent (GPT-5)
($) 1s)
Summary Agent (Qwen3-Coder-480B)
CodeAct (GPT-5) + BM25
CodeAct (Qwen3-Coder-480B) + BM25
2
CodeAct (GPT-5) + Subagents
2
CodeAct (Qwen3-Coder-480B) + Subagents
1
V
1
I
7
7
0
25th
50th
75th
95th
0
25th
50th
75th
95th
Percentile
Percentile
Figure 3. Cost of RLM and baselines described in §3.2 plotted at the 25th, 50th, 75th, and 95th percentile of total API cost. We observe
comparable or even lower costs for RLMs at the 5Oth percentile, but sharp increases at the tail end due to potentially long RLM trajectories.
instance, on BrowseComp-Plus (1K), a linearly extrapo-
benchmark can be loosely categorized by different process-
lated cost for GPT-5-mini ingesting 6-11M input tokens is
ing complexity of the input context with respect to length
$1.50 − $2.75, while RLM(GPT-5) has an average cost of
(roughly constant, linear, and quadratic respectively). In
$0.99 and outperforms both the summarization and retrieval
Figure 1, we directly compare an RLM using GPT-5 to base
baselines by over 29%.
GPT-5 on each task. We find that GPT-5 performance de-
grades significantly faster for more complex tasks, while
Furthermore, on tasks where processing costs scale with the
RLM performance degrades at a much slower rate, which
input context, RLMs make significant improvements over
aligns with the findings of Goldman et al. (2025). For con-
the base model, even on tasks within the model's context
text lengths beyond 214, the RLM consistently outperforms
window. On OOLONG, the RLM with GPT-5 and Qwen3-
GPT-5.
Coder outperform the base model by 28.4% and 33.3%
respectively. On OOLONG-Pairs, both GPT-5 and Qwen3-
Furthermore, RLM costs scale proportionally to the com-
Coder make little progress with F1 scores of <0.1%, while
plexity of the task, while still remaining in the same order of
the RLM using these models achieve F1 scores of 58.0% and
magnitude of cost as GPT-5 (see Figure 11 in Appendix F).
23.1% respectively, highlighting the emergent capability of
In §4.1, we explore the choices that the RLM makes that
RLMs to handle extremely information-dense tasks.
cause these differences in cost. Lastly, in this setting, we
Observation 2: The REPL is necessary for handling
also observe that the base LM outperforms RLM in the
small input context regime. By construction, a RLM has
long inputs, while the recursive sub-calling of RLMs
strictly more representation capacity than an LM. In prac-
provides strong benefits on information-dense inputs. A
tice, however, we observe that RLM performance is slightly
key characteristic of RLMs is offloading the context as a
worse on smaller input lengths, suggesting a tradeoff point
variable in an environment E that the model can interact
between when to use a base LM and when to use an RLM.
with. Even without sub-calling capabilities, our ablation of
the RLM is able to scale beyond the context limit of the
Observation 4: The inference cost of RLMs remains
model and outperform other task-agnostic baselines on most
comparable to a base LM call but has high variance
long context settings. On the CodeQA and BrowseComp+
due to differences in trajectory lengths. RLMs iteratively
tasks with Qwen3-Coder, this ablation is able to outperform
interact with their context until they find a suitable answer,
the RLM by 17.9% and 3% respectively.
leading to large differences in iteration length depending on
task complexity. In Figure 3, we plot the quartile costs for
On information-dense tasks like OOLONG or OOLONG-
each method across all experiments in Table 1 excluding
Pairs, we observed several cases where recursive LM sub-
BrowseComp-Plus (1K), as the base models cannot fit any
calling is necessary. In §4.1, we see RLM(Qwen3-Coder)
of these tasks in context. For GPT-5, the median RLM run
perform the necessary semantic transformation line-by-line
is cheaper than the median base model run, but many outlier
through recursive sub-calls, while the ablation without sub-
RLM runs are significantly more expensive than any base
calls is forced to use keyword heuristics to solve these tasks.
model query. However, compared to the summarization
Across all information-dense tasks, RLMs outperform the
agent which ingests the entire input context, RLMs are up to
ablation without sub-calling by 10%-59%.
3× cheaper while maintaining stronger performance across
Observation 3: LM performance degrades as a function
all tasks because the RLM is able to selectively view context.
of input length and problem complexity, while RLM
We additionally report runtime numbers of each method in
performance scales better. The benchmarks S-NIAH, OO-
Figures 7, 8 in Appendix F, but we note several important
LONG, and OOLONG-Pairs contain a fixed number of tasks
caveats. Unlike API costs, these numbers are heavily depen-
over contexts with lengths ranging from 213 to 218. Each
dent on implementation details such as the machine used,
6

## Page 7

Recursive Language Models
API request latency, and the asynchrony of LM calls. In our
on model priors. A key intuition for why the RLM ab-
implementation of the baselines and RLMs, all LM calls
straction can maintain strong performance on huge inputs
are blocking / sequential. Nevertheless, similar to costs, we
without exploding costs is the LM's ability to filter input
observe a wide range of runtimes, especially for RLMs.
context without explicitly seeing it. Furthermore, model
priors enable the RLM to narrow the search space and pro-
Observation 5: RLMs are a model-agnostic inference
cess fewer input tokens. As an example, in Figure 4a, we
strategy, but different models exhibit different overall
observed RLM(GPT-5) using regex queries to search for
decisions on context management and sub-calling. While
chunks containing keywords in the original prompt (e.g.
GPT-5 and Qwen3-Coder-480B both exhibit strong perfor-
"festival") and phrases it has a prior about (e.g. "La Union").
mance as RLMs relative to their base model and other base-
lines, they also exhibit different performance and behavior
Passing recursive LM outputs through variables for long
across all tasks. On BrowseComp-Plus (1k) in particular,
output tasks. RLMs are able to produce essentially un-
RLM(GPT-5) nearly solves all tasks while RLM(Qwen3-
bounded tokens well beyond the limit of the base LM by
Coder) struggles to solve half.
returning variables in the REPL as output. Through the
REPL, the RLM can iteratively construct these variables
We note that the RLM system prompt is fixed for each model
as a mixture of programmatic and sub-(R)LM output calls.
across all experiments and is not tuned for any particular
We observed this strategy used heavily in OOLONG-Pairs
benchmark. Between GPT-5 and Qwen3-Coder, the only
trajectories, where the RLM stored the output of sub-LM
difference in the prompt is an extra line in the RLM(Qwen3-
calls over the input in variables and stitched them together
Coder) prompt warning against using too many sub-calls
to form a final answer (see Figure 4c).
(see Appendix C). We provide an explicit example of this
difference in example E.3, where RLM(Qwen3-Coder)
launches a sub-call per line in OOLONG while GPT-5 is
5. Related Works
conservative about sub-querying LMs.
Long-Context LM Systems. There have primarily been
Observation 6: Training RLMs on one domain can im-
two orthogonal directions for long-context management
prove general downstream RLM performance. Certain
in language model systems: 1) directly changing the ar-
behavior in RLM trajectories are common among differ-
chitecture of and retraining the base LM to handle longer
ent domains, such as probing the input and recursively
contexts (Press et al., 2022; Gu et al., 2022; Munkhdalai
sub-calling on shorter contexts. In Table 1, we find that
et al., 2024), and 2) building a scaffold around the LM
RLM-Qwen3-8B, a Qwen3-8B model that we fine-tuned
that implicitly handles the context  RLMs focus on the
on RLM(Qwen3-Coder-480B-A35B) trajectories on a small,
latter. One popular class of such strategies is lossy context
unrelated set of tasks (LongBenchPro; Chen et al. 2026)
management, which uses summarization or truncation to
considerably outperforms the base Qwen3-8B as an RLM
compress the input context at the cost of potentially losing
by 28.3% on average. Furthermore, its inference costs are
fine-grained information. For example, MemWalker (Chen
much lower due to better decision making and fewer mis-
et al., 2023) constructs a tree-like data structure of the in-
takes as an RLM.
put that the LM can navigate when answering long context
questions. ReSum (Wu et al., 2025) is another work that
4.1. Emergent Patterns in RLM Trajectories
adds a summarization tool to periodically compress the
context of a multi-turn agent. Another class of strategies
Even without explicit training, RLMs exhibit interesting con-
implement an explicit memory hierarchy in the agent scaf-
text and problem decomposition behavior. We select several
fold (Packer et al., 2024; Chhikara et al., 2025; Zhang et al.,
examples of snippets from RLM trajectories to understand
2025). RLMs differ from these works in that all context
how they solve long context problems and where they can
window management is implicitly handled by the LM itself.
improve. We discuss particular examples of interesting be-
havior here, with additional examples in Appendix E.
Task Decomposition through sub-LM calls. Many LM-
based agents (Guo et al., 2024; Anthropic, 2025) use mul-
Chunking and recursively sub-calling LMs. RLMs defer
tiple, well-placed LM calls to solve a problem; however,
essentially unbounded-length reasoning chains to sub-LM
many of these calls are placed based on human-engineered
calls. The choice of decomposition can greatly affect task
workflows. Several methods like ViperGPT (Surís et al.,
performance, especially for information-dense problems.
2023), THREAD (Schroeder et al., 2025), DisCIPL (Grand
In our experiments, we did not observe complicated par-
et al., 2025), ReDel (Zhu et al., 2024), Context Folding (Sun
titioning strategies beyond uniform chunking or keyword
et al., 2025), and AgentFold (Ye et al., 2025) have explored
searches. In Figure 4b, RLM(Qwen3-Coder) chunks by
deferring the choice of sub-LM calls to the LM. These tech-
newline in a 1000+ line context from OOLONG.
niques emphasize task decomposition through recursive LM
Filtering input information using code execution based
calls, but are unable to handle long context inputs beyond
7

## Page 8

Recursive Language Models
# Let's process questions in batches to be more efficient
Execution 1
def
process_batch(questions_batch):
Execution Time: 0.158s
# Create a prompt for batch processing
prompt - "Classify each of the following questions into one of these 6 categori
prompt +=numeric value','entity', 'location'
'description and abstract cone
# Let's scan the context for clues using keyword searches and show snippets around
for i, question in enumerate(questions_batch):
def find_snippets(keyword, window=200, max_hits=10):
prompt += f"{i+1}. {question}\n"
hits - 
for i, chunk in enumerate(context):
start - 0
while True:
result - 11m_query(prompt)
idx = chunk.lowerO).find(keyword.lower(O, start)
return result.stripO).split('\n')
if idx -= -1:
break
(b) RLMs defer reasoning over its large
s = max(0, idx - window)
e = min(len(chunk), idx + len(keyword) + window)
context by querying recursive LM calls.
hits.append((i, chunk[s:e]))
if len(hits) >= max_hits:
start = idx + 1
return hits
return hits
keywords = ["dinengdeng", "pinakbet", "bagoong", "Agoo", "La Union", "festival",
# Create the final formatted result
results - {
formatted_pairs - [f"({pair[0]}, {pair[1]})" for pair in pairs]
for kw in keywords:
final_result - "\n".join(formatted_pairs)
results[kw] - find_snippets(kw, window400, max_hits5)
print(f"Total pairs in final result: {len(formatted_pairs)}")
for kw, hits in results.itemsO):
print(f"=-= Keyword: {kw}
print("First 5 pairs:")
if not hits:
print("\n".join(formatted_pairs[:5]))
print("No hits")
for
i, snip in hits:
FINAL_VAR(final_result)
print(f*[Chunk {i}]
. ..{snip} . . .\n")
Total pairs in final result: 10731
== Keyword: dinengdeng =.=
First 5 pairs:
[Chunk 6] ...
(12258, 12646)
(a) RLMs can interact with, probe, and filter the
(c) RLMs can stitch recursive LM outputs to
context using code (e.g. regex queries).
form longer, composite outputs.
Figure 4. RLMs have common patterns in their trajectories when solving tasks. (a) We frequently observed RLMs filtering and interacting
with their context through regex code. (b) We found that RLMs can effectively decompose their context through recursive sub-calls (c)
On long-output tasks, RLMs are able to solve sub-problems using recursive sub-LM calls and stitch their outputs to form a final output.
the length of the base LM. RLMs, on the other hand, are
as a RLM provides very rapid performance improvements,
enabled by an extremely simple intuition (i.e., placing the
even outside the training domain. We hypothesize that RLM
prompt in the external environment) to symbolically manip-
trajectories can be viewed as a form of reasoning (OpenAI
ulate arbitrarily long strings and to iteratively refine their
et al., 2024; DeepSeek-AI et al., 2025), which can be trained
recursion via execution feedback from the persistent REPL.
by bootstrapping existing models (Zelikman et al., 2022;
2024). We hope that training native RLMs can be treated as
6. Limitations and Future Work
a new axis of scale to improve LM performance on general
and long-horizon tasks.
While RLMs show strong performance on tasks beyond the
context window limitations of existing LMs at reasonable
7. Conclusion
inference costs, evaluations for more difficult and natural
long-context processing tasks and the best mechanisms for
We introduced Recursive Language Models (RLMs), a gen-
implementing RLMs both remain highly under-explored.
eral inference framework for language models that offloads
We focused on synchronous sub-calls inside of a Python
the input context and enables language models to recur-
REPL environment, but we note that alternative strategies in-
sively sub-query language models before providing an out-
volving asynchronous sub-calls and sandboxed REPLs can
put. We explored an instantiation of this framework that
potentially significantly reduce the runtime and inference
offloads the context into a Python REPL environment as
cost of RLMs. Furthermore, we chose to use a max recur-
a variable in memory, enabling the LM to reason over its
sion depth of one (i.e. sub-calls are LMs); while we found
context in code and recursive LM calls, rather than purely in
strong performance on existing long-context benchmarks,
token space. Our results across multiple settings and mod-
we believe that future work should investigate deeper levels
els demonstrated that RLMs are an effective task-agnostic
of recursion or even new hybrids between symbolic recur-
paradigm for both long-context problems and general rea-
sion and neural attention. We include additional limitations
soning. Building on our small fine-tuning experiments, we
and negative results in Appendix B.
are excited to see future work that explicitly trains models
to reason as RLMs, which could result in another axis of
Lastly, we focused our experiments on evaluating RLMs
scale for the next generation of language model systems.
using existing frontier models, but show initial evidence on a
Qwen3-8B model that explicitly training a model to be used
8

## Page 9

Recursive Language Models
8. Impact Statement
Chen, Z., Wu, X., Jia, J., Gao, C., Fu, Q., Zhang, D., and Hu,
S. Longbench pro: A more realistic and comprehensive
This paper explores a strategy for enabling language models
bilingual long-context evaluation benchmark, 2026. URL
to solve long context problems and scaling future language
https://arxiv.org/abs/2601.02872.
model systems. The goal is to advance research on systems
that can help us solve complex problems. While there are
Chhikara, P., Khant, D., Aryan, S., Singh, T., and Ya-
potential societal consequences of this work, we believe
dav, D. MemO: Building production-ready ai agents
they are not specific to this paper and do not need to be
with scalable long-term memory, 2025. URL ht tps :
highlighted here.
//arxiv.org/abs/2504.19413.
DeepSeek-AI, Guo, D., Yang, D., Zhang, H., Song, J.,
Acknowledgments
Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X.,
Zhang, X., Yu, X., Wu, Y., Wu, Z. F., Gou, Z., Shao,
This research is partially supported by the Laude Institute,
Z., Li, Z., Gao, Z., Liu, A., Xue, B., Wang, B., Wu, B.,
Prime Intellect, and Modal Labs. We thank Noah Ziems,
Feng, B., Lu, C., Zhao, C., Deng, C., Zhang, C., Ruan,
Jacob Li, James Moore, and the MIT OASYS and MIT DSG
C., Dai, D., Chen, D., Ji, D., Li, E., Lin, F., Dai, F., Luo,
labs for insightful discussions throughout this project. We
F., Hao, G., Chen, G., Li, G., Zhang, H., Bao, H., Xu,
also thank Jack Cook, Matej Sirovatka, Ofir Press, Sebastian
H., Wang, H., Ding, H., Xin, H., Gao, H., Qu, H., Li,
Müller, Simon Guo, and Zed Li for helpful feedback.
H., Guo, J., Li, J., Wang, J., Chen, J., Yuan, J., Qiu, J.,
Li, J., Cai, J. L., Ni, J., Liang, J., Chen, J., Dong, K.,
References
Hu, K., Gao, K., Guan, K., Huang, K., Yu, K., Wang, L.,
Anthropic.
Claude code: Subagents — modular
Zhang, L., Zhao, L., Wang, L., Zhang, L., Xu, L., Xia,
ai workflows with isolated agent contexts, 2025.
L., Zhang, M., Zhang, M., Tang, M., Li, M., Wang, M.,
URLhttps://docs.anthropic.com/en/docs/
Li, M., Tian, N., Huang, P., Zhang, P., Wang, Q., Chen,
claude-code/sub-agents.
Q., Du, Q., Ge, R., Zhang, R., Pan, R., Wang, R., Chen,
R. J., Jin, R. L., Chen, R., Lu, S., Zhou, S., Chen, S., Ye,
Bai, Y., Tu, S., Zhang, J., Peng, H., Wang, X., Lv, X.,
S., Wang, S., Yu, S., Zhou, S., Pan, S., Li, S. S., Zhou,
Cao, S., Xu, J., Hou, L., Dong, Y., Tang, J., and Li,
S., Wu, S., Ye, S., Yun, T., Pei, T., Sun, T., Wang, T.,
J. Longbench v2: Towards deeper understanding and
Zeng, W., Zhao, W., Liu, W., Liang, W., Gao, W., Yu, W.,
reasoning on realistic long-context multitasks, 2025. URL
Zhang, W., Xiao, W. L., An, W., Liu, X., Wang, X., Chen,
X., Nie, X., Cheng, X., Liu, X., Xie, X., Liu, X., Yang,
https://arxiv.org/abs/2412.15204.
X., Li, X., Su, X., Lin, X., Li, X. Q., Jin, X., Shen, X.,
Bertsch, A., Pratapa, A., Mitamura, T., Neubig, G., and
Chen, X., Sun, X., Wang, X., Song, X., Zhou, X., Wang,
X., Shan, X., Li, Y. K., Wang, Y. Q., Wei, Y. X., Zhang,
Gormley, M. R. Oolong: Evaluating long context rea-
soning and aggregation capabilities, 2025. URL ht tps :
Y., Xu, Y., Li, Y., Zhao, Y., Sun, Y., Wang, Y., Yu, Y.,
Zhang, Y., Shi, Y., Xiong, Y., He, Y., Piao, Y., Wang, Y.,
//arxiv.org/abs/2511.02817.
Tan, Y., Ma, Y., Liu, Y., Guo, Y., Ou, Y., Wang, Y., Gong,
Chang, Y., Lo, K., Goyal, T., and Iyyer, M. Booookscore: A
Y., Zou, Y., He, Y., Xiong, Y., Luo, Y., You, Y., Liu, Y.,
Zhou, Y., Zhu, Y. X., Xu, Y., Huang, Y., Li, Y., Zheng,
systematic exploration of book-length summarization in
the era of LLMs. In The Twelfth International Conference
Y., Zhu, Y., Ma, Y., Tang, Y., Zha, Y., Yan, Y., Ren, Z. Z.,
on Learning Representations, 2024. URL ht tps : / /
Ren, Z., Sha, Z., Fu, Z., Xu, Z., Xie, Z., Zhang, Z., Hao,
arxiv.org/pdf/2310.00785.pdf.
Z., Ma, Z., Yan, Z., Wu, Z., Gu, Z., Zhu, Z., Liu, Z., Li,
Z., Xie, Z., Song, Z., Pan, Z., Huang, Z., Xu, Z., Zhang,
Chen, H., Pasunuru, R., Weston, J., and Celikyilmaz,
Z., and Zhang, Z. Deepseek-r1: Incentivizing reasoning
A. Walking down the memory maze: Beyond context
capability in llms via reinforcement learning, 2025. URL
limit through interactive reading, 2023. URL ht tps :
https://arxiv.org/abs/2501.12948.
//arxiv.org/abs/2310.05029.
Fireworks AI.
Qwen3 coder 480b a35b instruct.
https://fireworks.ai/models/fireworks/
Chen, Z., Ma, X., Zhuang, S., Nie, P., Zou, K., Liu,
qwen3-coder-480b-a35b-instruct,2025.
A., Green, J., Patel, K., Meng, R., Su, M., Sharify-
moghaddam, S., Li, Y., Hong, H., Shi, X., Liu, X.,
Goldman, O., Jacovi, A., Slobodkin, A., Maimon, A., Da-
Thakur, N., Zhang, C., Gao, L., Chen, W., and Lin, J.
gan, I., and Tsarfaty, R. Is it really long context if all
Browsecomp-plus: A more fair and transparent evalu-
you need is retrieval? towards genuinely difficult long
ation benchmark of deep-research agent, 2025. URL
context nlp, 2025. URL https://arxiv.org/abs/
https://arxiv.org/abs/2508.06600.
2407.00402.
9